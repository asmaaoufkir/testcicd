input { pipeline { address => traces_purge } }

filter {

    #grok {  
    #    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp}  %{LOGLEVEL:log-level} %{GREEDYDATA:message}" } 
    #}

    # extract the json part of the message string into a separate field
    grok { 
        match => { "message" => "^.*?(?<logged_json>{.*)" } 
    }

    # parse the json and remove the string field upon success
    json { 
        source => "logged_json" 
        remove_field => [ "logged_json" ] 
    }
   
    mutate{	
            
        # mapping
        copy => { "[json_data][purge_step]" => "purge_step" }
        copy => { "[json_data][purge_mode]" => "purge_mode" }
        copy => { "[json_data][purge_balId]" => "purge_balId" }
        copy => { "[json_data][purge_frontal]" => "purge_frontal" }
        copy => { "[json_data][purge_taskId]" => "purge_taskId" }
        copy => { "[json_data][purge_before]" => "purge_before" }
        copy => { "[json_data][purge_nbMsgPurgeSuccessCount]" => "purge_nbMsgPurgeSuccessCount" }
        copy => { "[json_data][purge_nbMsgPurgeFailedCount]" => "purge_nbMsgPurgeFailedCount" }
        copy => { "[json_data][purge_nbMailboxesPurgeFailedCount]" => "purge_nbMailboxesPurgeFailedCount" }
        copy => { "[json_data][purge_result]" => "purge_result" }
        copy => { "[json_data][purge_resultAsString]" => "purge_resultAsString" }
        copy => { "[json_data][purge_duration]" => "purge_duration" }
        copy => { "[json_data][purge_durationTimeUnit]" => "purge_durationTimeUnit" }
        copy => { "[json_data][purge_msgError]" => "purge_msgError" }

        remove_field => [ "json_data" ] 
    }

}

# upsert to elasticsearch
output {
  elasticsearch {
    hosts => ["https://service.localdomain:9200"]
    user => "admin"
    password => "ChangezMoi"
    ssl_certificate_verification => false
    index => "msgsv-traces_batch_purge-%{+YYYY.MM.dd}"
  }
}
